---
published: true
---
If you have ever seen the transformers movie series you should probably saw, how they are transforming from a car to a robot or vice versa. Transformers in machine learning are similar to them

The best performing models also connect the encoder and decoder through an attention mechanism. 
Also,  the Transformers based solely on attention mechanism and encoder-decoder architecture

###  <p style="color:#A48D88";>Background</p>

Self-attention(sometimes called intra-ttention) is an attention mechanism relating different position of single sequence in order to compute a representation of the sequence

Tranformers is the first transduction model relying entirely on self-attention without using RNNs or 
convolution 

###  <p style="color:#A48D88";>Model Architecture</p>



---
published: true
---
If you have ever seen the transformers movie series you should probably saw, how they are transforming from a car to a robot or vice versa. Transformers in machine learning are similar to them

The best performing models also connect the encoder and decoder through an attention mechanism. 
Also,  the Transformers based solely on attention mechanism and encoder-decoder architecture

### Background

Self-attention(sometimes called intra-ttention) is an attention mechanism relating different position of single sequence in order to compute a representation of the sequence

Tranformers is the first transduction model relying entirely on self-attention without using RNNs or 
convolution 

### Model Architecture
Generally, Transformers made with encoder-decoder architecture but inside the encoder and decoder we have
some operation


|![_config.yml]({{ site.baseurl }}/images/Transformers/Model_Architecture.PNG)|
|:--:| 
| *Figure 1* |

